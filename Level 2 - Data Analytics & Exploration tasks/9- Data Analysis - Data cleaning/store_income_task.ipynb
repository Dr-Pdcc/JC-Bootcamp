{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqt_yzRy16Wj"
   },
   "source": [
    "## Compulsory Task \n",
    "\n",
    "In this compulsory task you will clean the country column and parse the date column in the **store_income_data_task.csv** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vBP3WN2O16Wp"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>store_name</th>\n",
       "      <th>store_email</th>\n",
       "      <th>department</th>\n",
       "      <th>income</th>\n",
       "      <th>date_measured</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Cullen/Frost Bankers, Inc.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>$54438554.24</td>\n",
       "      <td>4-2-2006</td>\n",
       "      <td>United States/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Nordson Corporation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tools</td>\n",
       "      <td>$41744177.01</td>\n",
       "      <td>4-1-2006</td>\n",
       "      <td>Britain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Stag Industrial, Inc.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>$36152340.34</td>\n",
       "      <td>12-9-2003</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>FIRST REPUBLIC BANK</td>\n",
       "      <td>ecanadine3@fc2.com</td>\n",
       "      <td>Automotive</td>\n",
       "      <td>$8928350.04</td>\n",
       "      <td>8-5-2006</td>\n",
       "      <td>Britain/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Mercantile Bank Corporation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Baby</td>\n",
       "      <td>$33552742.32</td>\n",
       "      <td>21-1-1973</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                   store_name         store_email  department  \\\n",
       "0   1   Cullen/Frost Bankers, Inc.                 NaN    Clothing   \n",
       "1   2          Nordson Corporation                 NaN       Tools   \n",
       "2   3        Stag Industrial, Inc.                 NaN      Beauty   \n",
       "3   4          FIRST REPUBLIC BANK  ecanadine3@fc2.com  Automotive   \n",
       "4   5  Mercantile Bank Corporation                 NaN        Baby   \n",
       "\n",
       "         income date_measured          country  \n",
       "0  $54438554.24      4-2-2006   United States/  \n",
       "1  $41744177.01      4-1-2006          Britain  \n",
       "2  $36152340.34     12-9-2003    United States  \n",
       "3   $8928350.04      8-5-2006         Britain/  \n",
       "4  $33552742.32     21-1-1973   United Kingdom  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import chardet as cd\n",
    "# Load up store_income_data.csv\n",
    "income_df = pd.read_csv('store_income_data_task.csv')\n",
    "\n",
    "# Display the first few rows to inspect the data\n",
    "income_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItqLwumA16Wr"
   },
   "source": [
    "1. Take a look at all the unique values in the \"country\" column. Then, convert the column to lowercase and remove any trailing white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sLkzt4Hr16Wr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique country values before cleaning: ['United States/' 'Britain' ' United States' 'Britain/' ' United Kingdom'\n",
      " 'U.K.' 'SA ' 'U.K/' 'America' 'United Kingdom' nan 'united states'\n",
      " ' S.A.' 'England ' 'UK' 'S.A./' 'ENGLAND' 'BRITAIN' 'U.K' 'U.K '\n",
      " 'America/' 'SA.' 'S.A. ' 'u.k' 'uk' ' ' 'UK.' 'England/' 'england'\n",
      " ' Britain' 'united states of america' 'UK/' 'SA/' 'SA' 'England.'\n",
      " 'UNITED KINGDOM' 'America.' 'S.A..' 's.a.' ' U.K'\n",
      " ' United States of America' 'Britain ' 'England' ' SA'\n",
      " 'United States of America.' 'United States of America/' 'United States.'\n",
      " 's. africasouth africa' ' England' 'United Kingdom '\n",
      " 'United States of America ' ' UK' 'united kingdom' 'AMERICA' 'America '\n",
      " 'UNITED STATES OF AMERICA' ' S. AfricaSouth Africa' 'america'\n",
      " 'S. AFRICASOUTH AFRICA' 'Britain.' '/' 'United Kingdom.' 'United States'\n",
      " ' America' 'UNITED STATES' 'sa' 'United States of America' 'UK '\n",
      " 'United States ' 'S. AfricaSouth Africa/' 'S.A.' 'United Kingdom/'\n",
      " 'S. AfricaSouth Africa ' 'S. AfricaSouth Africa.' 'S. AfricaSouth Africa'\n",
      " '.' 'britain']\n",
      "\n",
      "There are 77 unique values before cleaning.\n",
      "\n",
      "Unique country values after basic cleaning: ['united states' 'britain' 'united kingdom' 'u.k' 'sa' 'america' 's.a'\n",
      " 'england' 'uk' 'united states of america' 's. africasouth africa']\n",
      "\n",
      "There are 11 unique values after basic cleaning.\n"
     ]
    }
   ],
   "source": [
    "# Check the unique values before cleaning\n",
    "unique_before = income_df['country'].unique()\n",
    "print(\"Unique country values before cleaning:\", unique_before)\n",
    "print(f\"\\nThere are {len(unique_before)} unique values before cleaning.\")\n",
    "\n",
    "# Convert to lowercase\n",
    "income_df['country'] = income_df['country'].str.lower()\n",
    "\n",
    "# Remove white spaces\n",
    "income_df['country'] = income_df['country'].str.strip()\n",
    "\n",
    "# Remove trailing/leading spaces\n",
    "income_df['country'] = income_df['country'].str.rstrip('/.')\n",
    "income_df['country'] = income_df['country'].replace('', pd.NA)\n",
    "income_df.dropna(subset=['country'], inplace=True)\n",
    "\n",
    "# Check unique values after cleaning basic formatting\n",
    "unique_after = income_df['country'].unique()\n",
    "print(\"\\nUnique country values after basic cleaning:\", unique_after)\n",
    "print(f\"\\nThere are {len(unique_after)} unique values after basic cleaning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6dcDc4P16Ws"
   },
   "source": [
    "2. Note that there should only be three separate countries. Eliminate all variations, so that 'South Africa', 'United Kingdom' and 'United States' are the only three countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qeV3CxMR16Ws"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced the following matches with 'united states': ['united states', 'united states of america']\n",
      "Replaced the following matches with 'united kingdom': ['united kingdom']\n",
      "Replaced the following matches with 'south africa': ['s. africasouth africa']\n",
      "Unique country values after fuzzy matching: ['america', 'britain', 'england', 's.a', 'sa', 'south africa', 'u.k', 'uk', 'united kingdom', 'united states']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorot\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import process, fuzz\n",
    "\n",
    "\n",
    "def replace_matches_in_column(df, column, string_to_match, min_ratio = 60):\n",
    "    \"\"\"\n",
    "    Replace entries in df[column] that closely match string_to_match with string_to_match.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): The DataFrame containing the column to modify.\n",
    "        column (str): The name of the column to process.\n",
    "        string_to_match (str): The standardized string to replace close matches with.\n",
    "        min_ratio (int): The minimum fuzzy match ratio to consider for replacement.\n",
    "    \"\"\"\n",
    "    # Get the unique non-null strings from the column\n",
    "    unique_strings = df[column].dropna().unique()\n",
    "    \n",
    "    # Use fuzzy matching to extract the closest matches to string_to_match\n",
    "    matches = process.extract(string_to_match, unique_strings, scorer=fuzz.token_sort_ratio, limit=10)\n",
    "    \n",
    "    # Filter out matches that don't meet the minimum ratio\n",
    "    close_matches = [match[0] for match in matches if match[1] >= min_ratio]\n",
    "    \n",
    "    # Replace all rows that have any of these close matches with the standardised string\n",
    "    df.loc[df[column].isin(close_matches), column] = string_to_match\n",
    "    \n",
    "    print(f\"Replaced the following matches with '{string_to_match}': {close_matches}\")\n",
    "\n",
    "\n",
    "replace_matches_in_column(income_df, 'country', 'united states')\n",
    "replace_matches_in_column(income_df, 'country', 'united kingdom')\n",
    "replace_matches_in_column(income_df, 'country', 'south africa')\n",
    "\n",
    "# Verify the unique values after applying fuzzy matching\n",
    "unique_final = sorted(income_df['country'].dropna().unique())\n",
    "print(\"Unique country values after fuzzy matching:\", unique_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique country values after mapping: ['united states' 'united kingdom' 'south africa']\n"
     ]
    }
   ],
   "source": [
    "# Define a mapping for known country variants\n",
    "mapping = {\n",
    "    # United States variants\n",
    "    'america': 'united states',\n",
    "    \n",
    "    # United Kingdom variants\n",
    "    'britain': 'united kingdom',\n",
    "    'u.k': 'united kingdom',\n",
    "    'uk': 'united kingdom',\n",
    "    'england': 'united kingdom',\n",
    "    \n",
    "    # South Africa variants\n",
    "    's.a': 'south africa',\n",
    "    'sa': 'south africa'\n",
    "}\n",
    "\n",
    "# Apply the mapping\n",
    "income_df['country'] = income_df['country'].replace(mapping)\n",
    "\n",
    "# Verify the unique country values after mapping\n",
    "unique_final = income_df['country'].dropna().unique()\n",
    "print(\"Unique country values after mapping:\", unique_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJZDMTwP16Ws"
   },
   "source": [
    "3. Create a new column called `days_ago` in the DataFrame that is a copy of the 'date_measured' column but instead it is a number that shows how many days ago it was measured from the current date. Note that the current date can be obtained using `datetime.date.today()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMJbN84P16Wt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-06 00:00:00\n",
      "  date_measured date_parsed  days_ago\n",
      "0      4-2-2006  2006-02-04      6970\n",
      "1      4-1-2006  2006-01-04      7001\n",
      "2     12-9-2003  2003-09-12      7846\n",
      "3      8-5-2006  2006-05-08      6877\n",
      "4     21-1-1973  1973-01-21     19037\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "# First, parse the 'date_measured' column to datetime.\n",
    "income_df['date_parsed'] = pd.to_datetime(income_df['date_measured'], format='%d-%m-%Y')\n",
    "\n",
    "# Get today's date as a Python date, then convert to pandas Timestamp and normalise it\n",
    "today = pd.Timestamp(dt.date.today()).normalize()\n",
    "\n",
    "# Compute the number of days ago\n",
    "income_df['days_ago'] = (today - income_df['date_parsed']).dt.days\n",
    "\n",
    "# Display the result with the new column added\n",
    "print(income_df[['date_measured', 'date_parsed', 'days_ago']].head())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
